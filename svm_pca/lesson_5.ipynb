{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498a2e5",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Bayesian Gaussian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22550873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947 491 9 53\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# исходные параметры распределений двух классов\n",
    "r1 = 0.7\n",
    "D1 = 1.0\n",
    "mean1 = [1, -2]\n",
    "V1 = [[D1, D1 * r1], [D1 * r1, D1]]\n",
    "\n",
    "r2 = -0.5\n",
    "D2 = 2.0\n",
    "mean2 = [0, 2]\n",
    "V2 = [[D2, D2 * r2], [D2 * r2, D2]]\n",
    "\n",
    "# моделирование обучающей выборки\n",
    "N1 = 500\n",
    "N2 = 1000\n",
    "x1 = np.random.multivariate_normal(mean1, V1, N1).T\n",
    "x2 = np.random.multivariate_normal(mean2, V2, N2).T\n",
    "\n",
    "data_x = np.hstack([x1, x2]).T\n",
    "data_y = np.hstack([np.ones(N1) * -1, np.ones(N2)])\n",
    "\n",
    "# вычисление оценок МО и ковариационных матриц\n",
    "mm1 = np.mean(x1.T, axis=0)\n",
    "mm2 = np.mean(x2.T, axis=0)\n",
    "\n",
    "a = (x1.T - mm1).T\n",
    "VV1 = np.array([[np.dot(a[0], a[0]) / N1, np.dot(a[0], a[1]) / N1],\n",
    "                [np.dot(a[1], a[0]) / N1, np.dot(a[1], a[1]) / N1]])\n",
    "\n",
    "a = (x2.T - mm2).T\n",
    "VV2 = np.array([[np.dot(a[0], a[0]) / N2, np.dot(a[0], a[1]) / N2],\n",
    "                [np.dot(a[1], a[0]) / N2, np.dot(a[1], a[1]) / N2]])\n",
    "\n",
    "# для гауссовского байесовского классификатора\n",
    "Py1, L1 = 0.5, 1  # вероятности появления классов\n",
    "Py2, L2 = 1 - Py1, 1  # и величины штрафов неверной классификации\n",
    "\n",
    "func = lambda x, lm, py, m, cov: np.log(lm * py) - 1/2 * (x - m).T @ np.linalg.inv(cov) @ (x - m) - 1/2 * np.log(np.linalg.det(cov))\n",
    "predict = []\n",
    "classes = [-1, 1]\n",
    "TP = TN = FP = FN = 0\n",
    "for i in range(len(data_x)):\n",
    "    a = np.argmax([func(data_x[i], L1, Py1, mm1, VV1), func(data_x[i], L2, Py2, mm2, VV2)]) # return indexes (0 or 1)\n",
    "    predict.append(classes[a])\n",
    "    if classes[a] == 1 and data_y[i] == 1:\n",
    "        TP += 1\n",
    "    elif classes[a] == -1 and data_y[i] == -1:\n",
    "        TN += 1\n",
    "    elif classes[a] == 1 and data_y[i] == -1:\n",
    "        FP += 1\n",
    "    elif classes[a] == -1 and data_y[i] == 1:\n",
    "        FN += 1\n",
    "print(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f3fd1",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cadf5506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9722222222222222 0.7 35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# логарифмическая функция потерь\n",
    "def loss(w, x, y):\n",
    "\tmargin = np.dot(w, x) * y\n",
    "\treturn np.log2(1 + np.exp(-margin))\n",
    "\n",
    "# производная логарифмической функции потерь по вектору w\n",
    "def df(w, x, y):\n",
    "\tmargin = np.dot(w, x) * y\n",
    "\tnum = np.exp(-margin) * x.T * y \n",
    "\tden = np.log(2) * (1 + np.exp(-margin))\n",
    "\treturn - num / den\n",
    "\n",
    "data_x = [(5.8, 1.2), (5.6, 1.5), (6.5, 1.5), (6.1, 1.3), (6.4, 1.3), (7.7, 2.0), (6.0, 1.8), (5.6, 1.3), (6.0, 1.6), (5.8, 1.9), (5.7, 2.0), (6.3, 1.5), (6.2, 1.8), (7.7, 2.3), (5.8, 1.2), (6.3, 1.8), (6.0, 1.0), (6.2, 1.3), (5.7, 1.3), (6.3, 1.9), (6.7, 2.5), (5.5, 1.2), (4.9, 1.0), (6.1, 1.4), (6.0, 1.6), (7.2, 2.5), (7.3, 1.8), (6.6, 1.4), (5.6, 2.0), (5.5, 1.0), (6.4, 2.2), (5.6, 1.3), (6.6, 1.3), (6.9, 2.1), (6.8, 2.1), (5.7, 1.3), (7.0, 1.4), (6.1, 1.4), (6.1, 1.8), (6.7, 1.7), (6.0, 1.5), (6.5, 1.8), (6.4, 1.5), (6.9, 1.5), (5.6, 1.3), (6.7, 1.4), (5.8, 1.9), (6.3, 1.3), (6.7, 2.1), (6.2, 2.3), (6.3, 2.4), (6.7, 1.8), (6.4, 2.3), (6.2, 1.5), (6.1, 1.4), (7.1, 2.1), (5.7, 1.0), (6.8, 1.4), (6.8, 2.3), (5.1, 1.1), (4.9, 1.7), (5.9, 1.8), (7.4, 1.9), (6.5, 2.0), (6.7, 1.5), (6.5, 2.0), (5.8, 1.0), (6.4, 2.1), (7.6, 2.1), (5.8, 2.4), (7.7, 2.2), (6.3, 1.5), (5.0, 1.0), (6.3, 1.6), (7.7, 2.3), (6.4, 1.9), (6.5, 2.2), (5.7, 1.2), (6.9, 2.3), (5.7, 1.3), (6.1, 1.2), (5.4, 1.5), (5.2, 1.4), (6.7, 2.3), (7.9, 2.0), (5.6, 1.1), (7.2, 1.8), (5.5, 1.3), (7.2, 1.6), (6.3, 2.5), (6.3, 1.8), (6.7, 2.4), (5.0, 1.0), (6.4, 1.8), (6.9, 2.3), (5.5, 1.3), (5.5, 1.1), (5.9, 1.5), (6.0, 1.5), (5.9, 1.8)]\n",
    "data_y = [-1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1]\n",
    "\n",
    "x_train = np.array([[1, x[0], x[1]] for x in data_x])\n",
    "y_train = np.array(data_y)\n",
    "\n",
    "n_train = len(x_train)  # размер обучающей выборки\n",
    "w = np.array([0.0, 0.0, 0.0])  # начальные весовые коэффициенты\n",
    "nt = np.array([0.5, 0.01, 0.01])  # шаг обучения для каждого параметра w0, w1, w2\n",
    "lm = 0.01  # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "N = 500  # число итераций алгоритма SGD\n",
    "\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "TP = FP = FN = TN = 0\n",
    "for _ in range(N):\n",
    "\tk = np.random.randint(0, n_train-1)\n",
    "\tw = w -  nt * df(w, x_train[k], y_train[k])\n",
    "    \n",
    "a = lambda w, x: np.sign(np.dot(w, x))\n",
    "\n",
    "for i in range(n_train):\n",
    "\t\n",
    "\tif a(w, x_train[i]) == 1 and y_train[i] == 1:\n",
    "\t\tTP += 1\n",
    "\telif a(w, x_train[i]) == 1 and y_train[i] == -1:\n",
    "\t\tFP += 1\n",
    "\telif a(w, x_train[i]) == -1 and y_train[i] == 1:\n",
    "\t\tFN += 1\n",
    "\telif a(w, x_train[i]) == -1 and y_train[i] == -1:\n",
    "\t\tTN += 1\n",
    "\n",
    "precision = TP / (TP + FP) # точность\n",
    "recall = TP / (TP + FN) # полнота\n",
    "\n",
    "print(precision, recall, TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c15a3c",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Support Vector Machine with True/False Positive/Negative with F, F-beta scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ac62f",
   "metadata": {},
   "source": [
    "<img src=\".././photo/condition22.png\" alt=\"photo\" width=\"900\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2137cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112324492979719 0.8637873754152824\n",
      "0.8366854384553499\n",
      "0.8212255211623498\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# исходные параметры распределений классов\n",
    "r1 = 0.2\n",
    "D1 = 3.0\n",
    "mean1 = [2, -2]\n",
    "V1 = [[D1, D1 * r1], [D1 * r1, D1]]\n",
    "\n",
    "r2 = 0.5\n",
    "D2 = 2.0\n",
    "mean2 = [-1, -1]\n",
    "V2 = [[D2, D2 * r2], [D2 * r2, D2]]\n",
    "\n",
    "# моделирование обучающей выборки\n",
    "N1 = 2500\n",
    "N2 = 1500\n",
    "x1 = np.random.multivariate_normal(mean1, V1, N1).T\n",
    "x2 = np.random.multivariate_normal(mean2, V2, N2).T\n",
    "\n",
    "data_x = np.hstack([x1, x2]).T\n",
    "data_y = np.hstack([np.ones(N1) * -1, np.ones(N2)])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, random_state=123,test_size=0.4, shuffle=True)\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "predict = clf.predict(x_test)\n",
    "\n",
    "w_intercept = clf.intercept_[0]\n",
    "w_other = clf.coef_[0]\n",
    "w = np.hstack([w_intercept, w_other])\n",
    "# print(w)\n",
    "\n",
    "TN = TP = FN = FP = 0\n",
    "for i in range(len(x_test)):\n",
    "\n",
    "    if predict[i] == y_test[i] == 1:\n",
    "        TP += 1\n",
    "    elif predict[i] == y_test[i] == -1:\n",
    "        TN += 1\n",
    "    elif predict[i] == 1 and y_test[i] == -1:\n",
    "        FP += 1\n",
    "    elif predict[i] == -1 and y_test[i] == 1:\n",
    "        FN += 1\n",
    "\n",
    "precision = TP / (TP + FP) # the ratio of the true to all the positives of the model\n",
    "recall = TP / (TP + FN) # the ratio of the true to all the positives of the general\n",
    "print(precision, recall)\n",
    "\n",
    "# F score\n",
    "F = 2 * precision * recall / (precision + recall)\n",
    "# F-beta score (beta = 0.5 mean precision more consider)\n",
    "Fb = (1 + 1/2 ** 2) * precision * recall / (((1/2) ** 2) * precision + recall)\n",
    "\n",
    "print(F)\n",
    "print(Fb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
